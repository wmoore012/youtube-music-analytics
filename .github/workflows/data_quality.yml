name: Data Quality Validation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC (after data collection)
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Pipeline run type'
        required: true
        default: 'full'
        type: choice
        options:
        - full
        - quality_only
        - etl_only

jobs:
  production-pipeline:
    runs-on: ubuntu-latest
    
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: root_password
          MYSQL_DATABASE: yt_proj
          MYSQL_USER: etl_user
          MYSQL_PASSWORD: gidtik-gygfe3-kubKiz
        ports:
          - 3306:3306
        options: --health-cmd="mysqladmin ping" --health-interval=10s --health-timeout=5s --health-retries=3

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
    
    - name: Wait for MySQL
      run: |
        while ! mysqladmin ping -h"127.0.0.1" -P3306 -uroot -proot_password --silent; do
          sleep 1
        done
    
    - name: Set up test database
      env:
        DB_HOST: 127.0.0.1
        DB_PORT: 3306
        DB_USER: etl_user
        DB_PASS: gidtik-gygfe3-kubKiz
        DB_NAME: yt_proj
      run: |
        # Create test tables and sample data
        python tools/setup/create_tables.py
        
        # Insert minimal test data for quality checks
        python -c "
        from web.etl_helpers import get_engine
        import pandas as pd
        from datetime import datetime, timedelta
        
        engine = get_engine()
        
        # Create sample video data
        sample_videos = pd.DataFrame({
            'video_id': ['test_vid_1', 'test_vid_2', 'test_vid_3'],
            'title': [
                'Test Artist - Test Song [Official Music Video]',
                'Test Artist - Another Song [Official Audio]', 
                'Test Artist - Social Post'
            ],
            'channel_title': ['Test Artist', 'Test Artist', 'Test Artist'],
            'published_at': [datetime.now() - timedelta(days=1)] * 3,
            'isrc': [None, None, None]
        })
        
        sample_metrics = pd.DataFrame({
            'video_id': ['test_vid_1', 'test_vid_2', 'test_vid_3'],
            'metrics_date': [datetime.now().date()] * 3,
            'view_count': [1000, 500, 200],
            'like_count': [100, 50, 20],
            'comment_count': [10, 5, 2]
        })
        
        sample_videos.to_sql('youtube_videos', engine, if_exists='append', index=False)
        sample_metrics.to_sql('youtube_metrics', engine, if_exists='append', index=False)
        print('Test data inserted successfully')
        "
    
    - name: Run Production Pipeline
      env:
        DB_HOST: 127.0.0.1
        DB_PORT: 3306
        DB_USER: etl_user
        DB_PASS: gidtik-gygfe3-kubKiz
        DB_NAME: yt_proj
        PIPELINE_RUN_TYPE: ${{ github.event.inputs.run_type || 'full' }}
      run: |
        # Run the complete production pipeline
        python tools/etl/run_production_pipeline.py
        
        # Also run standalone quality check for comparison
        python scripts/run_data_quality_checks.py --output-format json > standalone_quality_results.json
    
    - name: Upload Pipeline Reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: pipeline-reports
        path: |
          production_pipeline_results.json
          standalone_quality_results.json
          production_pipeline.log
    
    - name: Run Notebook Tests
      env:
        DB_HOST: 127.0.0.1
        DB_PORT: 3306
        DB_USER: etl_user
        DB_PASS: gidtik-gygfe3-kubKiz
        DB_NAME: yt_proj
      run: |
        # Test that notebooks can load and process data without errors
        python -c "
        import sys
        sys.path.insert(0, '.')
        
        from web.etl_helpers import get_engine
        from src.youtubeviz.data import load_recent_window_days
        
        # Test data loading
        engine = get_engine()
        data = load_recent_window_days(days=30, engine=engine)
        print(f'✅ Data loading test passed: {len(data)} records')
        
        # Test song identification (from notebook)
        import re
        song_patterns = [
            r'\[Official Music Video\]',
            r'\[Official Audio\]'
        ]
        combined_pattern = '|'.join(song_patterns)
        data['is_song'] = data['video_title'].str.contains(combined_pattern, case=False, na=False)
        songs = data[data['is_song'] == True]
        print(f'✅ Song identification test passed: {len(songs)} songs found')
        
        # Test deduplication (natural keys)
        deduplicated = data.drop_duplicates(['video_id'], keep='first')
        duplicates_removed = len(data) - len(deduplicated)
        print(f'✅ Deduplication test passed: {duplicates_removed} duplicates removed')
        
        print('🎉 All notebook functionality tests passed!')
        "
    
    - name: Check for Critical Issues
      run: |
        # Parse results and fail if critical issues found
        python -c "
        import json
        import sys
        
        with open('data_quality_results.json', 'r') as f:
            results = json.load(f)
        
        if results['status'] == 'FAILED':
            print('💥 Critical data quality issues detected!')
            for error in results['errors']:
                print(f'   ❌ {error}')
            sys.exit(1)
        elif results['status'] == 'ERROR':
            print('💥 Data quality check errors!')
            for error in results['errors']:
                print(f'   🚫 {error}')
            sys.exit(1)
        else:
            print('✅ Data quality checks passed!')
        "

  notify-on-failure:
    runs-on: ubuntu-latest
    needs: data-quality
    if: failure()
    
    steps:
    - name: Notify on Failure
      run: |
        echo "🚨 Data Quality Check Failed!"
        echo "Please review the data quality report and fix any critical issues."
        echo "Check the workflow logs for detailed information."